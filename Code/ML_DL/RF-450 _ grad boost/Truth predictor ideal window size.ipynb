{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------ATTENTION: HERE CHANGE THE PATH DEPENDING ON YOUR DATA LOCATIONS -------\n",
    "basePath = \"/Users/macbookair/Desktop/università/computational intelligence/truth_predictor_project/models-data/data-from-eeglab/\"\n",
    "spectraBasePath = \"/Users/macbookair/Desktop/università/computational intelligence/truth_predictor_project/spectra_epoch/\"\n",
    "\n",
    "# data frame of all the time domain target data\n",
    "eegDataTarget = pd.DataFrame()\n",
    "# data frame of all the time domain NOT target data\n",
    "eegDataNotTarget = pd.DataFrame()\n",
    "\n",
    "# data frame of all the spectra target data\n",
    "spectraDataTarget = pd.DataFrame()\n",
    "# data frame of all the spectra NOT target data\n",
    "spectraDataNotTarget = pd.DataFrame()\n",
    "\n",
    "allSubjectList = os.listdir(basePath)\n",
    "\n",
    "# for all the subjects names contained in the directory \"basePath\"\n",
    "for subjectName in allSubjectList:\n",
    "    # if the subject is actually a directory\n",
    "    if os.path.isdir(basePath + subjectName):\n",
    "        # get the list of files names of a single subject directory\n",
    "        singleSubjectFiles = os.listdir(basePath + subjectName)\n",
    "        # for all files names in the subject directories\n",
    "        \n",
    "        for targetFileName in  singleSubjectFiles:\n",
    "            # if the file is a target file contains the string \"epIs\" and also if it ends with EEG.csv (that is time domain data)\n",
    "            if \"epIs\" in targetFileName and \"EEG.csv\" in targetFileName:\n",
    "                # put file name in target array\n",
    "                targetFile = pd.read_csv(basePath + subjectName + \"/\" + targetFileName,sep=\"\\t\")\n",
    "                \n",
    "                # concatenate the time domain TARGET data with the previously concatenated one coming from the other\n",
    "                # time domain EEG files taken in previous iterations of this for loop\n",
    "                eegDataTarget = pd.concat([eegDataTarget,targetFile])\n",
    "                \n",
    "                # with different path the NAME of a spectra file is equal to a \"time domain EEG file\", it changes\n",
    "                # only the fact that istead of EEG.CSV the name ends with SPECTRA.CSV\n",
    "                spectraFileNameTarget = spectraBasePath + targetFileName.replace(\"EEG.csv\",\"SPECTRA.csv\")\n",
    "                \n",
    "                # Get the spectra file\n",
    "                if os.path.exists(spectraFileNameTarget):\n",
    "                    spectraTargetFile = pd.read_csv(spectraFileNameTarget, header=None)\n",
    "                else:\n",
    "                    # in case the file does not exists there is a variant of SPECTRA FILE that ends with\n",
    "                    # EEG-SPECTRA.csv\n",
    "                    spectraTargetFile = pd.read_csv(spectraBasePath + targetFileName.replace(\"EEG.csv\",\"EEG-SPECTRA.csv\"), header=None)\n",
    "                \n",
    "                # concatenate the spectra TARGET data with the previously concatenated one coming from the other\n",
    "                # spectra files taken in previous iterations of this for loop\n",
    "                spectraDataTarget = pd.concat([spectraDataTarget,spectraTargetFile])\n",
    "                \n",
    "            elif \"epNot\" in targetFileName and \"EEG.csv\" in targetFileName:\n",
    "                # the logic is the same of the above else, but is about NOT Targets data\n",
    "                notTargetFile = pd.read_csv(basePath + subjectName + \"/\" + targetFileName,sep=\"\\t\")\n",
    "                eegDataNotTarget = pd.concat([eegDataNotTarget,notTargetFile])\n",
    "                spectraFileNameNotTarget = spectraBasePath + targetFileName.replace(\"EEG.csv\",\"SPECTRA.csv\")\n",
    "                \n",
    "                if os.path.exists(spectraFileNameNotTarget):\n",
    "                    spectraNotTargetFile = pd.read_csv(spectraFileNameNotTarget, header=None)\n",
    "                else:\n",
    "                    spectraNotTargetFile = pd.read_csv(spectraBasePath + targetFileName.replace(\"EEG.csv\",\"EEG-SPECTRA.csv\"), header=None)\n",
    "                    \n",
    "                spectraDataNotTarget = pd.concat([spectraDataTarget,spectraNotTargetFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a strange reason the time column has an empty name, so rename it to \"Time\" for target data\n",
    "eegDataTarget.rename(index=str, columns={\" \": \"Time\"},inplace=True)\n",
    "#for another strange reason an empty column is in the TARGET dataframe with name \"Unnamed: 5\" so let's remove it\n",
    "del eegDataTarget[\"Unnamed: 5\"]\n",
    "\n",
    "# same operations as above but for not targets\n",
    "eegDataNotTarget.rename(index=str, columns={\" \": \"Time\"},inplace=True)\n",
    "del eegDataNotTarget[\"Unnamed: 5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window start represents when we do we cut the data starting from the card shown event\n",
    "# e.g. 0-3000 means from the moment the card has been shown until 3 seconds after\n",
    "# REMEMBER THE DATA WINDOW HAS BEEN TAKEN WITH -1000:3000ms THIS ARE THE LIMITS OF THE WINDOW START\n",
    "# AND WINDOW END WE CAN HAVE\n",
    "windowStart = 0\n",
    "windowEnd = 3000\n",
    "\n",
    "# number of seconds of the windows (e.g. 3 seconds in case of an interval 0-3000ms)\n",
    "numberOfSeconds = (windowEnd - windowStart)/ 1000\n",
    "\n",
    "# elaboration of the number of sampled data for each round, that is 256 data points per second\n",
    "# in the e.g. 0-3000 it is: (3sec * 256sampling rate)\n",
    "numberOfSamplesPerRound = int(numberOfSeconds * 256)\n",
    "\n",
    "#cut the target and not target data with the decided window length\n",
    "eegDataTargetReduced = eegDataTarget[(eegDataTarget[\"Time\"] >= windowStart) & (eegDataTarget[\"Time\"] < windowEnd)]\n",
    "eegDataNotTargetReduced = eegDataNotTarget[(eegDataNotTarget[\"Time\"] >= windowStart) & (eegDataNotTarget[\"Time\"] < windowEnd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array which separates all the concatenated time domain data in different epochs, each epoch in the array\n",
    "# is an array containing all the samples data representing the epoch (e.g. data in 0-3000ms).\n",
    "# the creation of the array is useful to concatenate the EEG time domain data with the spectra, and for then \n",
    "# flatten it for the classifier by having this result as a row \"concatenate(EEG_subject_time_domain,subject_spectra)\"\"\n",
    "eegDataTargetReducedNumpy = np.array(eegDataTargetReduced.values)\n",
    "numberOfTargetValues = int(eegDataTargetReduced.shape[0]/numberOfSamplesPerRound)\n",
    "trainingTargetSet = np.array(np.array_split(eegDataTargetReducedNumpy, eegDataTargetReduced.shape[0]/numberOfSamplesPerRound))\n",
    "\n",
    "#do the same of above for not target\n",
    "eegDataNotTargetReducedNumpy = np.array(eegDataNotTargetReduced.values)\n",
    "trainingNotTargetSet = np.array(np.array_split(eegDataNotTargetReducedNumpy, eegDataNotTargetReduced.shape[0]/numberOfSamplesPerRound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same of above that has been done in time domain, this time is done for SPECTRA type data that will be\n",
    "# concatenated with the time domain\n",
    "spectraDataTargetReducedNumpy = np.array(spectraDataTarget.values)\n",
    "trainingSpectraTargetSet = np.array(np.array_split(spectraDataTargetReducedNumpy, spectraDataTarget.shape[0]/4))\n",
    "\n",
    "spectraDataNotTargetReducedNumpy = np.array(spectraDataNotTarget.values)\n",
    "trainingSpectraNotTargetSet = np.array(np.array_split(spectraDataNotTargetReducedNumpy, spectraDataNotTarget.shape[0]/4))\n",
    "spectraDataTargetReducedNumpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTargetSetReshaped = []\n",
    "trainingNotTargetSetReshaped = []\n",
    "#create the arrays of flattened data for TARGET time domain and spectra\n",
    "for i in range(numberOfTargetValues - 1):\n",
    "    trainingTargetSetReshaped.append(np.hstack((trainingTargetSet[i].flatten(),trainingSpectraTargetSet[i].flatten())))\n",
    "    \n",
    "#create the arrays of flattened data for NOT TARGET time domain and spectra\n",
    "for i in range(numberOfTargetValues - 1):\n",
    "    trainingNotTargetSetReshaped.append(np.hstack((trainingNotTargetSet[i].flatten(),trainingSpectraNotTargetSet[i].flatten())))\n",
    "    \n",
    "trainingTargetSetReshaped = np.array(trainingTargetSetReshaped)\n",
    "trainingNotTargetSetReshaped = np.array(trainingNotTargetSetReshaped)\n",
    "\n",
    "print(trainingTargetSetReshaped.shape)\n",
    "print(trainingNotTargetSetReshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "trainingTargetSetReshapedAndAveraged = np.zeros((trainingTargetSetReshaped.shape[0],trainingTargetSetReshaped.shape[1]))\n",
    "trainingNotTargetSetReshapedAndAveraged = np.zeros((trainingTargetSetReshaped.shape[0],trainingTargetSetReshaped.shape[1]))\n",
    "\n",
    "# this array can be used to check more window sizes for the smoothing made with the average, \n",
    "# the current best found window size is around 15, so to see one single risult for a smoothing of 15 on each size\n",
    "# set the variable to [15], otherwise you can try more window sizes in this way e.g.: [13,15,16,22,25]\n",
    "arrayOfWindowSizes = [15]\n",
    "\n",
    "for i in arrayOfWindowSizes:\n",
    "    numberOfElectrodesLinesToBeAveragedPerSide = i\n",
    "    \n",
    "    print(\"Window size: \",end=\"\")\n",
    "    print(numberOfElectrodesLinesToBeAveragedPerSide * 2)\n",
    "    \n",
    "    numberOfElectrodesLinesToBeAveraged = (numberOfElectrodesLinesToBeAveragedPerSide * 2) + 1\n",
    "    # the number of considered electrodes plus the time if in the data. The maximum value is \n",
    "    # 4 electrodes + time var = 5.\n",
    "    # if, for instance, we removed 2 electrodes it will be \n",
    "    # 2 electrodes + time = 3\n",
    "    # if we removed the time attribute from the data it will be \n",
    "    # 2 electrodes = 2\n",
    "    numberOfAttributesConsidered = 5\n",
    "    \n",
    "    # a data row is a set of sampled data flattened, to do the average there must be some jumps. To understand this\n",
    "    # consider the structure of two samples where each electrode is named E:\n",
    "    # (E1.1, E2.1, E3.1, E4.1, T1, E1.2, E2.2, E3.2, E4.2, T2)\n",
    "    # to make the average smoothing the index of E1.1 must be averaged with the index of E1.2 that is\n",
    "    # E1.1.index + numberOfAttributesConsidered, this reasoning has to be repeated for all the samples in all\n",
    "    # the time domain considered that was defined by startWindow and endWindow\n",
    "    totalNumberOfAttributesToJump = numberOfElectrodesLinesToBeAveragedPerSide * numberOfAttributesConsidered\n",
    "\n",
    "    # the data points considered are equal to the sampling rate * number of seconds in epoch * the number of attributes\n",
    "    timeDomainNumberOfDataFlattened = 256 * numberOfSeconds * numberOfAttributesConsidered\n",
    "    \n",
    "    # START AVERAGE SMOOTHING\n",
    "    # this code is relatively intricate and does the average smoothing for TARGET and NOT TARGEt\n",
    "    for i in range(trainingTargetSetReshaped.shape[0]):\n",
    "        for j in range(trainingTargetSetReshaped.shape[1]):\n",
    "            if j < timeDomainNumberOfDataFlattened and j >= totalNumberOfAttributesToJump and j < trainingTargetSetReshaped.shape[1] - totalNumberOfAttributesToJump:\n",
    "                z = 0\n",
    "                while z <= numberOfElectrodesLinesToBeAveragedPerSide:\n",
    "                    if z != 0:\n",
    "                        trainingTargetSetReshapedAndAveraged[i][j] += trainingTargetSetReshaped[i][j - (numberOfAttributesConsidered * z)]\n",
    "\n",
    "                    trainingTargetSetReshapedAndAveraged[i][j] += trainingTargetSetReshaped[i][j + (numberOfAttributesConsidered * z)]\n",
    "                    z = z + 1\n",
    "                trainingTargetSetReshapedAndAveraged[i][j] = trainingTargetSetReshapedAndAveraged[i][j] / numberOfElectrodesLinesToBeAveraged\n",
    "            else:\n",
    "                trainingTargetSetReshapedAndAveraged[i][j] = trainingTargetSetReshaped[i][j]\n",
    "\n",
    "\n",
    "    for i in range(trainingTargetSetReshaped.shape[0]):\n",
    "        for j in range(trainingTargetSetReshaped.shape[1]):\n",
    "            if j < timeDomainNumberOfDataFlattened and j >= totalNumberOfAttributesToJump and j < trainingNotTargetSetReshaped.shape[1] - totalNumberOfAttributesToJump:\n",
    "                z = 0\n",
    "                while z <= numberOfElectrodesLinesToBeAveragedPerSide:\n",
    "                    if z != 0:\n",
    "                        trainingNotTargetSetReshapedAndAveraged[i][j] += trainingNotTargetSetReshaped[i][j - (numberOfAttributesConsidered * z)]\n",
    "\n",
    "                    trainingNotTargetSetReshapedAndAveraged[i][j] += trainingNotTargetSetReshaped[i][j + (numberOfAttributesConsidered * z)]\n",
    "                    z = z + 1\n",
    "                trainingNotTargetSetReshapedAndAveraged[i][j] = trainingNotTargetSetReshapedAndAveraged[i][j] / numberOfElectrodesLinesToBeAveraged\n",
    "            else:\n",
    "                trainingNotTargetSetReshapedAndAveraged[i][j] = trainingNotTargetSetReshaped[i][j]\n",
    "                \n",
    "    #END AVERAGE SMOOTHING\n",
    "\n",
    "    # after averaging the variables are renamed\n",
    "    trainingTargetSetReshaped = trainingTargetSetReshapedAndAveraged\n",
    "    trainingNotTargetSetReshaped = trainingNotTargetSetReshapedAndAveraged\n",
    "\n",
    "    # the validation data considered is from 700 until the end\n",
    "    # for example if the amount of data rows we have is 804 the validations set will consist\n",
    "    # of 804 - 700 = 104 for targets and 104 for not targets\n",
    "    \n",
    "    validationSeparatorIndex = 700\n",
    "    validationTargetSetReshaped = trainingTargetSetReshaped[validationSeparatorIndex:-1]\n",
    "    validationNotTargetSetReshaped = trainingNotTargetSetReshaped[validationSeparatorIndex:-1]\n",
    "\n",
    "    #training data is formed by 700 data for targets + 700 for not targets = 1400\n",
    "    trainingTargetSetReshaped = trainingTargetSetReshaped[0:validationSeparatorIndex]\n",
    "    trainingNotTargetSetReshaped = trainingNotTargetSetReshaped[0:validationSeparatorIndex]\n",
    "\n",
    "    # a variable useful to state that there is equiprobability among target and not target\n",
    "    numberOfNotTarget = numberOfTargetValues\n",
    "\n",
    "    #concatenating target and not target in a unique set\n",
    "    trainingSet = np.concatenate((trainingTargetSetReshaped,trainingNotTargetSetReshaped))\n",
    "    \n",
    "    #creating and concatenating labels ones for target and zeros for not target\n",
    "    trainingSetLabelsOnes = np.ones((validationSeparatorIndex,), dtype=int)\n",
    "    trainingSetLabelsZeros = np.zeros((validationSeparatorIndex,), dtype=int)\n",
    "    trainingLabels = np.concatenate((trainingSetLabelsOnes,trainingSetLabelsZeros))\n",
    "\n",
    "    # initialize classifiers scores\n",
    "    rf450Score = 0\n",
    "    gradBoostScore = 0    \n",
    "\n",
    "    #scaling data with mean 0 and STD 1\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(trainingSet)\n",
    "    #reassigning the data scaled to the trainingSet variable\n",
    "    trainingSet = scaler.transform(trainingSet)\n",
    "\n",
    "    #if apply PCA is true Principal component analysis is applied to the dataset\n",
    "    applyPCA = False\n",
    "\n",
    "    if applyPCA:\n",
    "        #here fitting PCA\n",
    "        pca = PCA(.999).fit(trainingSet)\n",
    "        #transforming in lower dimensional space\n",
    "        trainingSet = pca.transform(trainingSet)\n",
    "\n",
    "    #number of folds for cross validation\n",
    "    numberOfFolds = 10\n",
    "\n",
    "    #for all the folds\n",
    "    for i in range(numberOfFolds):\n",
    "        #split the data in train, test, labels of training, and labels for test\n",
    "        train, test, labels, testLabels = train_test_split(trainingSet,trainingLabels,test_size=0.2,random_state=42)\n",
    "\n",
    "        # apply the data to the classifiers and sum up all the scores for later average\n",
    "        rf450 = RandomForestClassifier(n_estimators=450,n_jobs=-1)\n",
    "        rf450.fit(train, labels)\n",
    "        rf450Score += rf450.score(test,testLabels)\n",
    "        \n",
    "        gradBoost = GradientBoostingClassifier()\n",
    "        gradBoost.fit(train,labels)\n",
    "        gradBoostScore += gradBoost.score(test,testLabels)\n",
    "    \n",
    "    # print the average of the scores of the classifiers in terms of accuracy\n",
    "    print(\"grad boost score:\",end=\"\")\n",
    "    print(gradBoostScore/numberOfFolds)\n",
    "    print(\"grad boost conf:\")\n",
    "    print(confusion_matrix(testLabels, gradBoost.predict(test)))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"RF-450:\",end=\"\")\n",
    "    print(rf450Score/numberOfFolds)\n",
    "    print(\"\")\n",
    "    \n",
    "    # concatenate the validation target and not target in a unique set\n",
    "    validationSet = np.concatenate((validationTargetSetReshaped,validationNotTargetSetReshaped))\n",
    "\n",
    "    # apply the scaling to the validation data\n",
    "    validationSet = scaler.transform(validationSet)\n",
    "\n",
    "    #apply also PCA if necessary\n",
    "    if applyPCA:\n",
    "        validationSet = pca.transform(validationSet)\n",
    "\n",
    "    #create the validation labels\n",
    "    labelsNumber = int(validationSet.shape[0]/2)\n",
    "    validationSetLabelsOnes = np.ones((labelsNumber,), dtype=int)\n",
    "    validationSetLabelsZeros = np.zeros((labelsNumber,), dtype=int)\n",
    "    validationLabels = np.concatenate((validationSetLabelsOnes,validationSetLabelsZeros))\n",
    "\n",
    "    #print the validation results\n",
    "    print(\"Grad boost validation score:\",end=\"\")\n",
    "    print(gradBoost.score(validationSet,validationLabels))\n",
    "    print(\"grad boost validation conf:\")\n",
    "    print(confusion_matrix(validationLabels, gradBoost.predict(validationSet)))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"RF-450 validation score:\",end=\"\")\n",
    "    print(rf450.score(validationSet,validationLabels))\n",
    "    print(\"RF conf:\")\n",
    "    print(confusion_matrix(validationLabels, rf450.predict(validationSet)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
